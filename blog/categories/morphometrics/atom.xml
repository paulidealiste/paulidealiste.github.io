<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Morphometrics | Creative morphometrics]]></title>
  <link href="http://paulidealiste.github.io/blog/categories/morphometrics/atom.xml" rel="self"/>
  <link href="http://paulidealiste.github.io/"/>
  <updated>2014-05-17T22:05:47+02:00</updated>
  <id>http://paulidealiste.github.io/</id>
  <author>
    <name><![CDATA[Miloš Blagojević]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Complete Procrustes in Python]]></title>
    <link href="http://paulidealiste.github.io/blog/2014/05/17/complete-procrustes-in-python/"/>
    <updated>2014-05-17T20:38:16+02:00</updated>
    <id>http://paulidealiste.github.io/blog/2014/05/17/complete-procrustes-in-python</id>
    <content type="html"><![CDATA[<p>Procrustes superimposition is the first analytic step in geometric morphometrics and this post shows one possible solution to performing it in Python<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>, using several functions defined in previous posts. First steps include data generation and definition of functions important for extracting shape variables from randomly generated landmark data. These functions are <em>centsize</em> which calculates centroid size from a numpy array (xy data) and <em>transScale</em> which translates landmark coordinates to the origin of the coordinate system and scales them to unit centroid size. The <em>mshapr</em> function is needed in the main superimposition function since it calculates succesive mean shapes (all configurations), excluding the configuration that is currently being rotated.</p>

<p>``` r Import libraries, function definition and data generation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy.spatial.distance as sd</p>

<p>def centsize(M): #centroid size
  p = M.shape[0]
  csize = np.sqrt(np.sum(M.var(0))*(p-1))
  return csize</p>

<p>def transScale(M):  #translation and scale
  tM = M &ndash; M.mean()
  centSize = centsize(M)
  tM = tM / centSize
  return tM</p>

<p>def mshapr (pdf):  #which is a pandas DataFrame, coordinates and individuals columns
  meanShapes = pd.DataFrame()
  dimen = len(allCoords.individuals.unique())
  for ind in range(0, dimen):</p>

<pre><code>temp = pdf[pdf.individuals != ind]
meanShape = temp.iloc[:,0:2].groupby(temp.iloc[:,2]).mean()
meanShapes = meanShapes.append(meanShape)
</code></pre>

<p>  return meanShapes</p>

<p>coordstest = np.vstack([np.random.uniform(175, 220, 10),   #usual data generation</p>

<pre><code>                    np.random.uniform(175, 220, 10)]).T 
</code></pre>

<p>coords = np.vstack([np.random.multivariate_normal(coordstest[i,:], [[3,0],[0,3]], 200) for i in range(10)])
coordinates = pd.Series((np.arange(0,10).reshape(-1,1)<em>np.ones(200).reshape(1,-1)).flatten()) #coordinates column
individuals = np.tile(np.arange(0,200),10) #individuals column
allCoords = pd.DataFrame(coords, columns = [&lsquo;x&rsquo;,&lsquo;y&rsquo;])
allCoords[&lsquo;coordinates&rsquo;] = coordinates
allCoords[&lsquo;individuals&rsquo;] = individuals
allCoords = allCoords.sort_index(by = [&lsquo;individuals&rsquo;,&lsquo;coordinates&rsquo;])
```
Definition of the </em>pProc<em> function follows the same rules as in the previous post, with the robust (and ugly for now) if clause that is only included to allow either pandas DataFrame or a numpy array as the input data (both for configurations and mean shape objects). It also returns only the rotated matrix, landmark configuration (</em>pmat1*) and not the mean shape.</p>

<p>``` r pProc function for two configuration matrices
def pProc(mat1, mat2): #returning only centred preshape of mat1 on mat2
  if type(mat1) is np.ndarray and type(mat2) is np.ndarray:</p>

<pre><code>k = mat1.shape[1]-1
m = mat1.shape[0]
sscaledMat1 = mat1 / centsize(mat1)
sscaledMat2 = mat2 / centsize(mat2)
z1 = sscaledMat1 - [sscaledMat1[:,0].mean(), sscaledMat1[:,1].mean()]
z2 = sscaledMat2 - [sscaledMat2[:,0].mean(), sscaledMat2[:,1].mean()]
tempSv = np.dot(np.transpose(z2), z1)
svdMat = np.linalg.svd(tempSv)
U = svdMat[0]
V = svdMat[2]
Ds = svdMat[1]
tempSig = np.linalg.det(np.dot(np.transpose(z1), z2))
sig = np.sign(tempSig)
Ds[k] = sig * np.absolute(Ds[k])
U[:,k] = sig * U[:,k]
Gam = np.dot(V, np.transpose(U))
beta = np.sum(Ds)
pmat1 = np.dot(z1, Gam)
pmat1 = pd.DataFrame(pmat1, columns = ['x','y'])
return pmat1
</code></pre>

<p>  else:</p>

<pre><code>mat1x = mat1.iloc[:,0]
mat1y = mat1.iloc[:,1]
mat1 = np.concatenate([mat1x, mat1y], axis = 1).reshape(mat1.shape[1],mat1.shape[0]).T
mat2x = mat2.iloc[:,0]
mat2y = mat2.iloc[:,1]
mat2 = np.concatenate([mat2x, mat2y], axis = 1).reshape(mat2.shape[1],mat2.shape[0]).T
k = mat1.shape[1]-1
m = mat1.shape[0]
sscaledMat1 = mat1 / centsize(mat1)
sscaledMat2 = mat2 / centsize(mat2)
z1 = sscaledMat1 - [sscaledMat1[:,0].mean(), sscaledMat1[:,1].mean()]
z2 = sscaledMat2 - [sscaledMat2[:,0].mean(), sscaledMat2[:,1].mean()]
tempSv = np.dot(np.transpose(z2), z1)
svdMat = np.linalg.svd(tempSv)
U = svdMat[0]
V = svdMat[2]
Ds = svdMat[1]
tempSig = np.linalg.det(np.dot(np.transpose(z1), z2))
sig = np.sign(tempSig)
Ds[k] = sig * np.absolute(Ds[k])
U[:,k] = sig * U[:,k]
Gam = np.dot(V, np.transpose(U))
beta = np.sum(Ds)
pmat1 = np.dot(z1, Gam)
pmat1 = pd.DataFrame(pmat1, columns = ['x','y'])
return pmat1
</code></pre>

<p>```</p>

<p>Finally, the <em>pGP</em> function, that can perform partial Procrustes superimposition for any number of individuals (landmark configurations) and landmarks. Generated data has 200 individuals and 10 2D landmarks. For now this function will ask such information in the function call, so that mmat1 is pandas DataFrame with x, y, coordinates and individuals columns (generated above), numind is the number of individuals, dim is 2D (3D not yet supported), and numland is the number of landmarks. For clearer understanding of the following code, comments are included where appropriate.</p>

<p>``` r Partial Procrustes Superimposition
def pGP (mmat1, numind, dim, numland):
  groupCoords = mmat1.iloc[:,0:2].groupby(mmat1.iloc[:,3])
  transCoords = groupCoords.apply(transScale) #translate and scale the sample data
  individuals = np.tile(np.arange(0,numind),numland)
  coordinates = pd.Series((np.arange(0,numland).reshape(-1,1)<em>np.ones(numind).reshape(1,-1)).flatten())
  transCoords[&lsquo;coordinates&rsquo;] = coordinates
  transCoords[&lsquo;individuals&rsquo;] = np.sort(individuals).astype(str)
  #this part of the code calculates Q-the convergence criterion
  #that is really the sum of the pairwise squared distances between all shapes in the sample
  #so that, after rotation these distances are minimized as much as possible
  arrayx = transCoords.iloc[:,0]
  arrayy = transCoords.iloc[:,1]
  forDist = np.concatenate([arrayx, arrayy], axis = 1).reshape(dim,numland</em>numind).T
  forDist = forDist.reshape(numind, numland*2) #distance function from scipy
  Qm1 = sd.pdist(forDist)
  Q = Qm1.sum()
  while absolute(Q) > 0.00001: #execute the following code until Q cannot be reduced anymore</p>

<pre><code>groupTrans = transCoords.iloc[:,0:2].groupby(transCoords.iloc[:,3])
meanShapes = mshapr(transCoords) #calculate DataFrame of mean shapes excluding one individual at the time
meanShapes['individuals'] = np.sort(individuals).astype(str)
meanGroups = meanShapes.iloc[:,0:2].groupby(meanShapes.iloc[:,2])   
tempRot = pd.DataFrame()
#following for loop performs partial Procrustes superimposition (pProc function from above) between
#mean shapes and each configuration (individual)
for ind in range(0, numind): 
  tosto = pProc(groupTrans.get_group(str(ind)), meanGroups.get_group(str(ind)))
  tempRot = tempRot.append(tosto)
tempX = tempRot.iloc[:,0]
tempY = tempRot.iloc[:,1]
tempDist = np.concatenate([tempX, tempY], axis = 1).reshape(dim,numland*numind).T
tempDist = forDist.reshape(numind, numland*2)
Qm2 = sd.pdist(tempDist)
Q = Qm1.sum() - Qm2.sum()
Qm1 = Qm2 #re-calculate the convergence criterion at each step
transCoords = tempRot
</code></pre>

<p>  return tempRot
```</p>

<p>DataFrame <em>tempRot</em> holds the superimposed configurations (shape variables) that can, subsequently, be used in standard GM analyses and further. Of course, graphical display of superimposition results can be very interesting, and in this post only basic plots will be given, while some of the further posts may include more visualizations. Figure 1 shows the original raw-generated data, Figure 2 the spatial relationships between raw and superimposed data, while Figure 3 shows only superimposed data.</p>

<p>```r Plotting the data and the relationship of raw and superimposed data
procCoords = pGP(allCoords, 200,2,10) #perform superimposition
procoordinates = np.tile(np.arange(0,10),200)
procCoords[&lsquo;coordinates&rsquo;] = procoordinates
meanShape1 = allCoords.iloc[:,0:2].groupby(allCoords.iloc[:,2]).mean() #calculate mean shape for raw data
meanShape1 = pd.DataFrame(polarRotator(np.array(meanShape1))) #natural ordering for landmark labels
plt.scatter(allCoords[&lsquo;x&rsquo;], allCoords[&lsquo;y&rsquo;], s = 30, c = &ldquo;#82CDFF&rdquo;, edgecolors = &lsquo;none&rsquo;) #plot original sampled points
plt.scatter(meanShape1[0], meanShape1[1], s = 50, c = &ldquo;#7909E8&rdquo;, edgecolors = &lsquo;none&rsquo;)
plt.plot(meanShape1[0], meanShape1[1], &lsquo;&ndash;&rsquo;, color = &ldquo;#7909E8&rdquo;)
labels = [&lsquo;Landmark {0}&rsquo;.format(i) for i in range(10)]
for label, x, y in zip(labels, meanShape1[0], meanShape1[1]): #annotate mean landmarks by numbers
  plt.annotate(label, xy = (x, y+0.4),ha = &lsquo;right&rsquo;, va = &lsquo;bottom&rsquo;)
plt.grid()
meanShape2 = procCoords.iloc[:,0:2].groupby(procCoords.iloc[:,2]).mean() #calculate mean shape for superimposed data
meanShape2 = pd.DataFrame(polarRotator(np.array(meanShape2))) #natural ordering for landmark labels
plt.scatter(procCoords[&lsquo;x&rsquo;], procCoords[&lsquo;y&rsquo;], s = 30, c = &ldquo;#FFD699&rdquo;, edgecolors = &lsquo;none&rsquo;)
plt.scatter(meanShape2[0], meanShape2[1], s = 50, c = &ldquo;#7909E8&rdquo;, edgecolors = &lsquo;none&rsquo;)
plt.plot(meanShape2[0], meanShape2[1], &lsquo;&ndash;&rsquo;, color = &ldquo;#7909E8&rdquo;)
for label, x, y in zip(labels, meanShape2[0], meanShape2[1]): #annotate mean landmarks by numbers
  plt.annotate(label, xy = (x, y+0.01),ha = &lsquo;right&rsquo;, va = &lsquo;bottom&rsquo;)
plt.grid()</p>

<h1>for Figure 2 just plt.scatter of both allCoords and procCoords in the same window</h1>

<p>```
<img class="center" src="/images/rawWithout.png" width="650" height="484" title="&lsquo;raw generated landmark data&rsquo;" >
<img class="center" src="/images/originalSuperimposed.png" width="650" height="484" title="&lsquo;original vs superimposed&rsquo;" >
<img class="center" src="/images/superWith.png" width="650" height="481" title="&lsquo;superimposed landmark data&rsquo;" ></p>
<div class="footnotes">
<hr/>
<ol>
<li id="fn:1">
<p>If using IPython (:)) best way is to paste code with the %paste magic function.<a href="#fnref:1" rev="footnote">&#8617;</a></p></li>
</ol>
</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Modularity, Graphs and Triangulations]]></title>
    <link href="http://paulidealiste.github.io/blog/2014/05/03/modularity-graphs-and-triangulations/"/>
    <updated>2014-05-03T00:08:25+02:00</updated>
    <id>http://paulidealiste.github.io/blog/2014/05/03/modularity-graphs-and-triangulations</id>
    <content type="html"><![CDATA[<p>Continuing on the previous post, procedures presented here show the comparison between the usual assesment of modularity in the collection of landmarks (same dataset as before) and the depiction of modular structure through graph theory. Usually, modularity in the mammalian cranium is determined a-priori, according to some of the established hypotheses about the developmental origin of cranial elements. One of the most prevalent hypotheses is the two-module organization, one module comprising of elements originating mainly from the neural crest embryonic tissue (anterior cranium) and one comprising of elements with somitomeric origin (posterior cranium). Following code shows the test of this specific hypothesis using <em>geomorph</em> package functions, and additional usage of <em>deldir</em> for visualization of modularity hypotheses.</p>

<p>```r Importing data, basic GM, modularity and plots
library(geomorph)
library(deldir)
library(ggplot2)
load(&ldquo;capSample.RData&rdquo;)
theme_set(theme_bw())</p>

<p>capGPA &lt;&ndash; gpagen(capSample)
capCoords &lt;&ndash; capGPA$coords
mshapeCap &lt;&ndash; mshape(capCoords) #mean shape is useful for plotting</p>

<h1>vector depicting the division of landmarks according to modularity</h1>

<p>modularity1.gps &lt;&ndash; c(&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;A&rdquo;,&ldquo;B&rdquo;,&ldquo;B&rdquo;,&ldquo;A&rdquo;,&ldquo;B&rdquo;,&ldquo;B&rdquo;,&ldquo;B&rdquo;,&ldquo;B&rdquo;,&ldquo;B&rdquo;)</p>

<h1>test for modularity based on the RV coefficient</h1>

<p>modularity1 &lt;&ndash; compare.modular.partitions(capCoords, landgroups = modularity1.gps)</p>

<h1>plot of landmark membership to a-priori modules</h1>

<p>deldir(mshapeCap[,1], mshapeCap[,2], plotit = TRUE, wlines = &ldquo;triang&rdquo;, xlim = c(-0.2,0.55))
points(mshapeCap[c(1:8,11),1], mshapeCap[c(1:8,11),2], col = &ldquo;#FF8740&rdquo;, pch = 19, cex = 1.2)
points(mshapeCap[c(9,10,12:16),1], mshapeCap[c(9,10,12:16),2], col = &ldquo;#61B4CF&rdquo;, pch = 19, cex = 1.2)
legend(0.2,0.4, legend = c(&ldquo;neural crest&rdquo;, &ldquo;somitomere&rdquo;), fill = colormap)
```</p>

<p>Figure 1 shows the theoretical distribution of the RV coefficients, calculated for all posible two-module subdivisions of landmarks, as well as the observed RV value from the hypothesis depicted in Figure 2. Since the observed value is lower than most of the hypothetic values, then it could be said that this modularity hypothesis stands.</p>

<p><img class="center" src="/images/RVmodularity.png" width="570" height="497" title="&lsquo;RV coefficients&rsquo;" >
<img class="center" src="/images/modularityLand.png" width="344" height="376" title="&lsquo;Modularity hypothesis&rsquo;" ></p>

<p>The idea for using graph theory representation and analysis of modularity is based on correlation matrix, same one from the previous post. Correlation matrix was obtained through Delaunay triangulation and every vertex (node) in the graph actually represents the 1/3 of the total area of all Delaunay triangles emanating from the corresponding landmark.</p>

<p><code>r Delaunay triangulation and derivation of the correlation matrix
cicoCap &lt;- t(apply(capCoords, 3, function(A) deldir(A[,1],A[,2])$summary[,4]))
cicoCap &lt;- data.frame(cicoCap)
names(cicoCap) &lt;- paste("lm", c(1:16), sep = "")
capCor &lt;- abs(cor(cicoCap)) #correlation matrix
diag(capCor) &lt;- 0 #diagonals must be set to zero
</code></p>

<p>Graph based on the correlation matrix can be created and manipulated with the wonderful <em>igraph</em> package. Vertices of this graph will be abovementioned areas, while the edges will represent correlation (edge weights) between areas around each landmark. According to weights, edges can be colored and the strength of thier lines increased, so that the most correlated landmarks wolud be more obvious.</p>

<p><code>r iGraph graph creation and manipulation
graph &lt;- graph.adjacency(capCor, weighted=TRUE, mode="upper")
E(graph)[ weight &gt; 0.5 ]$color &lt;- "#A63E00"
E(graph)[ weight &gt; 0.5 ]$width &lt;- 3.6
E(graph)[ weight &gt; -0.5 &amp; weight &lt; 0.5 ]$color &lt;- "#D6EBFF"
E(graph)[ weight &gt; -0.5 &amp; weight &lt; 0.5 ]$width &lt;- 1.2
</code></p>

<p>Modularity, or community structure in the graph theory represents the degree of compartmentalization in the graph structure, based on the spatial relationship or the weights of graph edges. Although, a-priori subdivision of graph vertices is possible, and the evaluation of such graph community can be easily done, the advantage of graph theory for modularity is the availability of algorithms for searching the community structure, so it can derive subdivsion a-posteriori, that can be compared to the hypothesis from Figure 1.</p>

<p>```r Graph modularity and plotting
gsc &lt;&ndash; leading.eigenvector.community(graph, weights = E(graph)$weights)
modularity(graph, membership(gsc))
colormap &lt;&ndash; c(&ldquo;#FF8740&rdquo;, &ldquo;#61B4CF&rdquo;)
V(graph)$color &lt;&ndash; colormap[gsc$membership] #color graph vertices according to community</p>

<h1>graph layout will be circular since values for the edges range from 0.2 to 0.9,</h1>

<h1>and they all form similar attractive force between the vertices.</h1>

<p>graph$layout &lt;&ndash; layout.fruchterman.reingold #really no effect on the shape of this graph</p>

<p>plot(graph, vertex.size = 20, vertex.shape = &ldquo;circle&rdquo;)
legend(0.8,1.4, legend = c(&ldquo;neural crest&rdquo;, &ldquo;somitomere&rdquo;), fill = colormap)
```</p>

<p>Leading eigenvector community algorithm tries to find community structure in the graph by calculating the eigenvector of the modularity matrix for the largest positive eigenvalue and then separating vertices into two communities based on the sign of the corresponding element in the eigenvector (Newman, 2006). This method was preffered over many others because it is closer to usual multivariate methods, such as PCA, that are commonly used to depict variability patterns.</p>

<p><img class="center" src="/images/modularityGraph.png" width="541" height="467" title="&lsquo;Modularity graph&rsquo;" ></p>

<p>Graph community strucure depicts modularity in the landmark configurations accurately, with the exception of lm16, that is positioned on the posterior basicranium. Higher correlations are, on average, present within modules while between modules, only one connection is higher than 0.5, that between lm2 and lm14. Since these landmarks lay on the opposite sides of the cranium, they may encompass the variability in total anterior-posterior length. i.e. cranial size.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Delaunay Triangulation Experiment]]></title>
    <link href="http://paulidealiste.github.io/blog/2014/04/04/delaunay-triangulation-for-modularity-pt-1/"/>
    <updated>2014-04-04T18:09:37+02:00</updated>
    <id>http://paulidealiste.github.io/blog/2014/04/04/delaunay-triangulation-for-modularity-pt-1</id>
    <content type="html"><![CDATA[<p>Since landmark data consists of x and y Cartesian coordinates, it would be useful if they could be represented by one summary number, that would preserve the spatial information. This is just a proposal of one possiblity using Delaunay triangulation between landmark configurations, and the average area of all triangles emanating from individual landmarks. Since landmark position within the triangulation grid would influence the shape and size as well as number of triangles around it, the area should preserve enoguh information about the spatial relationships of landmarks. Dataset for this post consists of ventral aspect half configurations from <a href="http://goo.gl/LP3xsd" target="_blank">here</a>. Delaunay triangulation will be performed using <em>deldir</em> R package, since it reports the mentioned triangle areas as a part of object summary.</p>

<p>```r Importing data, basic GM, calculating the triangulation, extracting areas and plotting
library(deldir)
library(geomorph)
load(&ldquo;capSample.RData&rdquo;)</p>

<p>capreolusGPA &lt;&ndash; gpagen(capSample)
capGPAcoords &lt;&ndash; capreolusGPA$coords</p>

<p>delCap &lt;&ndash; deldir(capSample[,,10][,1], capSample[,,10][,2]) #extract any individual from an array for visualization
plot(delCap, col=c(&ldquo;lightblue&rdquo;,&ldquo;lightgrey&rdquo;,1,1,1), lwd = c(2,2), xlim = c(150, 400), cex = 0.1, ann = FALSE)
points(capSample[,,10], col = &ldquo;orange&rdquo;, pch = 16, cex = 1.4)
text(capSample[,,10], label = c(1:16), pos = 3, ceo = 0.5)</p>

<p>areaCap &lt;&ndash; t(apply(capGPAcoords, 3, function(A) deldir(A[,1],A[,2])$summary[,4])) #calcuate areas per landmark
areaCap &lt;&ndash; data.frame(areaCap)
names(areaCap) &lt;&ndash; paste(&ldquo;lm&rdquo;, c(1:16), sep = &ldquo;&rdquo;)
```</p>

<p><img class="center" src="/images/Delaunay.png" width="401" height="473" title="&lsquo;Delaunay&rsquo;" ></p>

<p>Values in the areaCap matrix represent average areas of all triangles emanating from the landmark in question, so its dimensions are 60x16. Correlation matrix (16x16) can be calculated from the areaCap matrix, and it should reflect relative positioning of landmarks through spatial grouping pattern. This can be checked visually using <em>corrplot</em> package visualization of the correlation matrix, which allows direct assesment of emerging patterns in the matrix. Additionally, this package allows reordering of rows and columns according to hierarchical clustering algorhithms and representing the desired number of clusters with rectangles in the correlation plot. This can only be considered as an idea of general modularity, since landmarks from the same cranial region should be correlated more than distant landmarks. But the transformation of xy coordinates to average triangle areas may have introduced non-biological variation or obscured some of the natural variation and these procedures as well as subsequent analyses may be treated only as a fun experiment and visualization tool for now.</p>

<p><code>r Drawing correlation plot
library(corrplot)
capCor &lt;- cor(areaCap)
corrplot(capCor, method = "circle", tl.cex = 0.93, order = "hclust", addrect = 3)
</code></p>

<p><img class="center" src="/images/Corrplot.png" width="570" height="529" title="&lsquo;Delaunay&rsquo;" ></p>

<p>Correlation plot with three proposed general landmark groups in the matrix reveals that the correlations are, on average, higher for locally grouped landmaks, especially the anterior ones, from 1 to 7. This can`t be used as a reliablie test for modularity hypothesis, but it can serve as a basis for further analyses based on construcing linked graphs or networks using correlation matrices from landmark data, where the xy coordinates are transformed to one number. Also this can be a useful way of representing modular structure visually, since both Delaunay and correlation plots are highly customizable, and can be colored according to real modularity hypotheses. Testing some of these will be the subject of future posts.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python Forcing Monsters` Shape]]></title>
    <link href="http://paulidealiste.github.io/blog/2014/03/08/python-forcing-monsters%60-shape/"/>
    <updated>2014-03-08T11:49:21+01:00</updated>
    <id>http://paulidealiste.github.io/blog/2014/03/08/python-forcing-monsters`-shape</id>
    <content type="html"><![CDATA[<p>Continuing on one of the previous posts about data generation in python, next natural step in the analytic procedure is the Procrustes superimposition. Since this procedure enables direct analyses of configurations` shape, and all subsequent explorative visualizations, it must be employed first. This post concerns with the basic superimposition procedure, involving only two landmark configurations, i.e. two generated monsters. One monster will be used as a reference and one as a target, which should undergo superimposition. First steps in data genration are the same as before, with the exception of polarRotator function that can take any numpy array and reorder it according to polar rotation angle. This is very useful since generated landmarks are not ordered properly and do not &ldquo;feel natural&rdquo; in plots and analyses.</p>

<p>```r Library import, data generation and some functions</p>

<p>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</p>

<p>def centsize(M):   #centroid size of any configuration matrix
  p = M.shape[0]
  csize = np.sqrt(np.sum(M.var(0))*(p-1))
  return csize</p>

<p>def polarRotator(M):   #polar rotation for natural ordering of landmarks
  x = np.array(M[:,0])
  y = np.array(M[:,1])
  points = np.array((x,y)).T
  cent = (np.sum(x)/len(M), np.sum(y)/len(M))
  angle = np.arctan2(points[:,1]-cent[1],points[:,0]-cent[0])
  points = pd.DataFrame(points, columns = [&lsquo;x&rsquo;,&lsquo;y&rsquo;])
  angle = pd.Series(angle)
  points[&lsquo;angle&rsquo;] = angle
  points = points.sort(&lsquo;angle&rsquo;)
  rotatedMat = np.array(points[[&lsquo;x&rsquo;,&lsquo;y&rsquo;]])
  return rotatedMat</p>

<p>coordstest = np.vstack([np.random.uniform(175, 220, 10), np.random.uniform(175, 220, 10)]).T
monster1 = np.vstack([np.random.multivariate_normal(coordstest[i,:], [[3,0],[0,3]], 1) for i in range(10)])
monster2 = np.vstack([np.random.multivariate_normal(coordstest[i,:], [[3,0],[0,3]], 1) for i in range(10)])</p>

<p>monster1 = polarRotator(monster1)
monster2 = polarRotator(monster2)
```
The plot of configurations reveals their spatial relationship, as well as the general mean shape. This time, since landmarks are ordered properly, one line would be enough for representing mean shapes, and shapes of respective configurations.</p>

<p>```r Plotting the original monsters</p>

<p>plt.scatter(monster1[:,0], monster1[:,1], s = 80, c = &ldquo;#FFD200&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;monster1&rdquo;)
plt.plot(monster1[:,0], monster1[:,1], &lsquo;&mdash;&rsquo;, color = &ldquo;#FFD200&rdquo;)
plt.scatter(monster2[:,0], monster2[:,1], s = 80, c = &ldquo;#47BFDD&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;monster2&rdquo;)
plt.plot(monster2[:,0], monster2[:,1], &lsquo;&mdash;&rsquo;, color = &ldquo;#47BFDD&rdquo;)</p>

<p>meanMon = (monster1 + monster2) / 2 #calculate mean shape
plt.scatter(meanMon[:,0], meanMon[:,1], s = 90, c = &ldquo;#6a12c4&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;mean monster&rdquo;)
plt.plot(meanMon[:,0], meanMon[:,1], &lsquo;&ndash;&rsquo;, color = &ldquo;#6a12c4&rdquo;)</p>

<p>labels = [&lsquo;Landmark {0}&rsquo;.format(i) for i in range(10)]
for label, x, y in zip(labels, meanMon[:,0], meanMon[:,1]): #annotate mean landmarks by numbers
  plt.annotate(label, xy = (x, y+0.3), ha = &lsquo;right&rsquo;, va = &lsquo;bottom&rsquo;)</p>

<p>plt.grid()
plt.legend(loc = &ldquo;lower left&rdquo;)
```</p>

<p><img class="center" src="/images/example1preProc.png" width="616" height="462" title="&lsquo;pre Procrustes 1&rsquo;" >
<img class="center" src="/images/example2preProc.png" width="616" height="457" title="&lsquo;pre Procrustes 2&rsquo;" ></p>

<p>Procrustes superimposition revolves around three features of shape extraction, that is invariance of landmark configurations to position, scale and rotation. There are a number of excelent textbooks about the mathematics and logic, as well as procedures for Procrustes superimposition (Bookstein, 1991, Dryden and Mardia, 1998, Zelditch et al., 2012), but for this post direct inspiration was <em>Morphometrics in R</em> (Claude, 2008), especially with the basic procedure presented in the following function definition.</p>

<p>```r Partial Procrustes superimposition of two configurations</p>

<p>def pProc(mat1, mat2):
  k = mat1.shape[1]-1
  m = mat1.shape[0]
  sscaledMat1 = mat1 / centsize(mat1) #scaling and centering
  sscaledMat2 = mat2 / centsize(mat2)
  z1 = sscaledMat1 &ndash; [sscaledMat1[:,0].mean(), sscaledMat1[:,1].mean()]
  z2 = sscaledMat2 &ndash; [sscaledMat2[:,0].mean(), sscaledMat2[:,1].mean()]
  tempSv = np.dot(np.transpose(z2), z1) #rotation
  svdMat = np.linalg.svd(tempSv)
  U = svdMat[0]
  V = svdMat[2]
  Ds = svdMat[1]
  tempSig = np.linalg.det(np.dot(np.transpose(z1), z2))
  sig = np.sign(tempSig)
  Ds[k] = sig * np.absolute(Ds[k])
  U[:,k] = sig * U[:,k]
  Gam = np.dot(V, np.transpose(U))
  beta = np.sum(Ds)
  pmat1 = np.dot(z1, Gam)
  pmat2 = z2
  tempRot = vstack((pmat1, pmat2))
  rotatedMat = pd.DataFrame(tempRot, columns = [&lsquo;x&rsquo;,&lsquo;y&rsquo;])
  return rotatedMat
<code>``
Plot of the superimposed configurations reveals that the Procrustes python was really able to force monsters</code> shape be more similar, removing the effects of orientation, size and rotation.</p>

<p>```r Plot of the superimposed configurations</p>

<p>rotatedMon = pProc(monster1, monster2)
rotatedMon1 = np.array(rotatedMon[:10])
rotatedMon2 = np.array(rotatedMon[10:20])</p>

<p>plt.scatter(rotatedMon1[:,0], rotatedMon1[:,1], s = 80, c = &ldquo;#FFD200&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;super monster1&rdquo;)
plt.plot(rotatedMon1[:,0], rotatedMon1[:,1], &lsquo;&mdash;&rsquo;, color = &ldquo;#FFD200&rdquo;)
plt.scatter(rotatedMon2[:,0], rotatedMon2[:,1], s = 80, c = &ldquo;#47BFDD&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;super monster2&rdquo;)
plt.plot(rotatedMon2[:,0], rotatedMon2[:,1], &lsquo;&mdash;&rsquo;, color = &ldquo;#47BFDD&rdquo;)</p>

<p>meanRotMon = (rotatedMon1 + rotatedMon2) / 2
plt.scatter(meanRotMon[:,0], meanRotMon[:,1], s = 90, c = &ldquo;#6a12c4&rdquo;, edgecolors = &lsquo;none&rsquo;, label = &ldquo;super mean monster&rdquo;)
plt.plot(meanRotMon[:,0], meanRotMon[:,1], &lsquo;&ndash;&rsquo;, color = &ldquo;#6a12c4&rdquo;)</p>

<p>labelsRot = [&lsquo;Landmark {0}&rsquo;.format(i) for i in range(10)]
for label, x, y in zip(labelsRot, meanRotMon[:,0], meanRotMon[:,1]): #annotate mean landmarks by numbers
  plt.annotate(label, xy = (x, y+0.01), ha = &lsquo;right&rsquo;, va = &lsquo;bottom&rsquo;)</p>

<p>plt.grid()
plt.legend(loc = &ldquo;lower left&rdquo;)
```</p>

<p><img class="center" src="/images/example1Proc.png" width="616" height="454" title="&lsquo;Procrustes monster&rsquo;" >
<img class="center" src="/images/example2Proc.png" width="616" height="452" title="&lsquo;Procrustes monster 2&rsquo;" ></p>

<p>Following posts should continue on this one and describe how the partial Procrustes superimposition for multilple configurations can be performed with fabulous sientific python.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[R Generator and a Colorful PCA]]></title>
    <link href="http://paulidealiste.github.io/blog/2014/02/23/r-generator-and-a-colorful-pca/"/>
    <updated>2014-02-23T11:24:16+01:00</updated>
    <id>http://paulidealiste.github.io/blog/2014/02/23/r-generator-and-a-colorful-pca</id>
    <content type="html"><![CDATA[<p>As simple as it may seem, sample data generation is not a trivial task, especially when random landmarks are to be generated. Usually, one would use multivariate normal distribution-based generator (like mvrnorm in R`s MASS package) in order to generate correlated data. The following function was used to generate data similar to the real world datased, unfortunately based directly on it, by using the coefficients from regressions between successive columns in a data matrix, which represent landmark coordinates in XY data matrix. The following function uses <a href="http://goo.gl/ijI1kn" target="_blank">this</a> data matrix (446 individuals and 28 landmarks), and performs regressions between X-Y pairs for all coordinates. Finally it uses intercepts and slopes to infer mean and SD for rnorm function, random number generator.</p>

<p>```r Resampler function for random resamples of a real data matrix</p>

<p>resampler &lt;&ndash; function(mat) #simulations based on rnorm random sampling
{
  x &lt;&ndash; dim(mat)[1]
  y &lt;&ndash; dim(mat)[2]
  indexrow &lt;&ndash; c(2:x)
  combinations &lt;&ndash; matrix(c(1:y,2:y), ncol = 2) #ignore the warning message
  slope &lt;&ndash; numeric(y)
  intercept &lt;&ndash; numeric(y)
  sds &lt;&ndash; numeric(y)
  for(i in 1:y)
  {</p>

<pre><code>veca &lt;- mat[,combinations[i,]][,1]
vecb &lt;- mat[,combinations[i,]][,2]
model &lt;- lm(veca~vecb)
slope[i] &lt;- coef(model)[2]
intercept[i] &lt;- coef(model)[1]
sds[i] &lt;- sd(mat[,combinations[i,]][,1])
</code></pre>

<p>  }
  coefs &lt;&ndash; data.frame(intercept,slope,sds)
  cexox &lt;&ndash; data.frame(c(1:x))
  for (i in 1:y)
  {</p>

<pre><code>dataCol &lt;- rnorm(length(mat[,combinations[i,]][,2]),mean=intercept[i]+slope[i]*mat[,combinations[i,]][,2],sd=sds)
cexox &lt;- cbind(cexox, dataCol)
</code></pre>

<p>  }
  sampleMatrix &lt;&ndash; as.matrix(cexox)
  sampleMatrix &lt;&ndash; sampleMatrix[,-1]
  return(sampleMatrix)
}</p>

<p>resampledCap &lt;&ndash; resampler(capreolusMatrix) #resample the original matrix-generate random coordinates
```
When the function finishes the output is also an XY matrix, which needs to be converted to an array in order to use it in gpagen function from the <em>geomorph</em> package. After that the procedure follows all the usual steps of the GM analysis, with the exception of factor levels generation in order to simulate grouping, and finally performing PCA on the Procrustes shape variables.</p>

<p><code>r Basic GM procedures and factor level generation
library(geomorph)
capreolusArray &lt;- arrayspecs(resampledCap, 28, 2, byLand = FALSE)
capreolusGPA &lt;- gpagen(capreolusArray, ShowPlot = FALSE)
pop &lt;- sample(5, 446, replace = TRUE) #generate random 5 population partition
pop[which(pop == 1)] &lt;- "pop1"
pop[which(pop == 2)] &lt;- "pop2"
pop[which(pop == 3)] &lt;- "pop3"
pop[which(pop == 4)] &lt;- "pop4"
pop[which(pop == 5)] &lt;- "pop5"
capreolusGPA2d &lt;- two.d.array(capreolusGPA$coords) #get the data in XY format for PCA
</code></p>

<p>PCA is then done using the usual R`s prcomp function and <em>ggplot2</em> for plotting the data points using fantastic <a href="http://colorbrewer2.org/" target="_blank">ColorBrewer</a> color schemes (which are the names of types and palettes in <em>ggplot2</em> scale_color_brewer geom). In order to fine-tune the PCA figure, the ggplot2 can also use custom fonts for plot annotation. Prior to that, fonts must be imported and registered, which is greatly facilitated by using the <em>extrafont</em> library.</p>

<p>```r PCA and ggplot2 code for a PCA scatterplot
capPCAwhole &lt;&ndash; prcomp(capreolusGPA2d)
capPCA &lt;&ndash; data.frame(capPCAwhole$x[,1], capPCAwhole$x[,2], capPCAwhole$x[,3], capPCAwhole$x[,4])
capPCA &lt;&ndash; data.frame(capPCA, pop)
names(capPCA) &lt;&ndash; c(&ldquo;PC1&rdquo;,&ldquo;PC2&rdquo;,&ldquo;PC3&rdquo;,&ldquo;PC4&rdquo;,&ldquo;pop&rdquo;) #prepare a data.frame for ggplot2
meanPCA1 &lt;&ndash; aggregate(capPCA[,1], mean, by = list(capPCA[,5])) #calculate average PC score per group for plotting
meanPCA2 &lt;&ndash; aggregate(capPCA[,2], mean, by = list(capPCA[,5]))
meanPCA &lt;&ndash; data.frame(meanPCA1, meanPCA2[,2])
names(meanPCA) &lt;&ndash; c(&ldquo;pop&rdquo;,&ldquo;PC1&rdquo;,&ldquo;PC2&rdquo;)</p>

<p>library(extrafont) #for using i.e. Times New Roman Fonts in ggplots
font_import(pattern=&ldquo;[T/t]imes&rdquo;) #this imports Times font family
loadfonts(device=&ldquo;pdf&rdquo;)</p>

<p>library (ggplot2)
theme_set(theme_bw())
pcaplot &lt;&ndash; ggplot(capPCA, aes(x=PC1, y=PC2, group = pop)) + geom_point(size = 7, shape = 19, aes(color=pop)) + scale_color_brewer(palette=&ldquo;Set1&rdquo;)
pcaplot &lt;&ndash; pcaplot + theme(panel.grid.major = element_line(size = 0.8, linetype = 2)) + theme(panel.grid.minor = element_line(size = 1, linetype = 2))
pcaplot &lt;&ndash; pcaplot + theme(text=element_text(size=20, family=&ldquo;Times New Roman&rdquo;), legend.text=element_text(size = 22, family = &ldquo;Times New Roman&rdquo;), legend.title = element_text(family =&ldquo;Times New Roman&rdquo;)) + xlab(&ldquo;PC1&rdquo;) + ylab(&ldquo;PC2&rdquo;)
pcaplot &lt;&ndash; pcaplot + geom_point(data = meanPCA, size = 14, shape = 19) + geom_text(data = meanPCA, size = 10, label = meanPCA$pop, family = &ldquo;Times New Roman&rdquo;, vjust = -0.9)
pcaplot
```</p>

<p><img class="center" src="/images/pcaplot.png" width="616" height="466" title="&lsquo;PCA&rsquo;" ></p>

<p>The plot indicates very little differentiation between the populations, but I guess that`s well expected since so much randomness is at hand.</p>
]]></content>
  </entry>
  
</feed>
